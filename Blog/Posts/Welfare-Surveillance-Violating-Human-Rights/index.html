<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
    <title>Welfare Surveillance Violating Human Rights</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Blog post about Welfare Surveillance Violating Human Rights">
    <meta name="keywords" content="Blog, Web Development, Machine Learning, AI">
    <meta name="author" content="Thishen Packirisamy 1839434">
    <script src="/WSOA3028A_1839434/Source/js/navbar.js" type="module" async></script>
    <link rel="stylesheet" href="/WSOA3028A_1839434/Source/CSS/styles.css">

</head>





<body>
    <header>
    </header>
    <article class="h-entry container">
        <header>
            <h2 class="p-name">Welfare Surveillance Violating Human Rights</h2>
        </header>
        <section class="e-content">
            <p>
                An article from the Guardian titled <cite>Welfare surveillance system violates human rights, Dutch court
                    rules</cite>[1] describes that AI enabled surveillance systems used to detect welfare fraud has been
                banned
                due to human rights violations. The article goes on to state the lack of transparency about the system.
                This brings up the possibility for privacy concerns as there is no way of knowing who has access to the
                large amount of data being collected on these individuals. The lack of transparency also means possible
                biases could be showing up in the algorithm whether intended or not and the public would have no way to
                know.
            </p>
            <p>
                This use case for machine learning technology is unethical for a many reasons. One reason would be the
                fact that the system will have a huge impact on peoples lives. The next reason is the fact that machine
                learning algorithms are not known to be accurate in situations regarding human nature but rather in
                situations in which clear much simpler data sets. Human behaviour is much more complex than a machine
                learning algorithm can predict. Another problem with this use case is the fact that machine learning
                algorithms form patterns by iterating upon variables in using minimisation algorithms. The programers
                therefore do not take part in this part of the process but only feed the system a large amount of data.
                This means that if the algorithm starts to display unwanted characteristics the programmer must retrain
                the system and can't make small tweaks to algorithm in order to rectify the problem. It is therefore a
                black box type of algorithm. Since programmers don't have much control over the way the system makes
                it's choices the system will perform well on given data. This is an issue due to the fact that much data
                collected throughout history has had biases. This is because it is being collected in a society which
                already has a large amount of inequality and preconsieved biases. Since the actual data put in is biased
                the systems will perform in a biased way. This is evident by a study conducted by a part of the US
                government called the National Institute of standards and technology[2]. This study highlighted the
                racial bias that was evident in many facial recognition algorithms with substantial difference in
                accuracy when identifying caucasian faces as opposed to faces of all other races. Perhaps the key
                problem with this use case is the fact that it is using a machine learning system to basically
                generalise people based on given data. Generalisation of human beings who are all incredibly unique is a
                problem when coming from from other human beings and to use a machine learning system does not make it
                any less discriminatory.
            </p>
        </section>
        <section class="References">
            <h2>References</h2>
            <p>
                <cite class="h-cite">
                    [1]Henley and Booth (2020), 'Welfare surveillance system violates human right', The Gaurdian,
                    https://www.theguardian.com/technology/2020/feb/05/welfare-surveillance-system-violates-human-rights-dutch-court-rules,
                    Accessed 03/06/2020
                </cite>
            </p>
            <p>
                <cite class="h-cite">
                    [2]Jan Wolfe, Jeffrey Dastin, ”U.S. government study finds racial bias in facial recognition tools”,
                    Reuters,
                    https://www.reuters.com/article/us-usa-crime-face/u-s-government-study-findsracial-bias-in-facial-recognition-tools-idUSKBN1YN2V1,
                    Accessed 04/06/2020
                </cite>
            </p>
        </section>

        <footer>
            <p>Written by <a class="p-author h-card" href="https://github.com/ThishenP">Thishen Packirisamy</a>
            <p class="dt-published">Posted: 04/06/2020</p>
        </footer>
    </article>
</body>

</html>